name: Performance Regression Test

on:
  pull_request:
    paths:
      - 'Assets/uPiper/Native/OpenJTalk/src/**'
  workflow_dispatch:

jobs:
  performance-test:
    name: Performance Test - ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
    
    steps:
    - uses: actions/checkout@v4
      with:
        lfs: true
        
    - uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - uses: lukka/get-cmake@latest
      
    - name: Build and Benchmark
      run: |
        cd Assets/uPiper/Native/OpenJTalk
        chmod +x build_ci.sh
        ./build_ci.sh
        
        # Run benchmark multiple times for stability
        cd build
        echo "=== Running Performance Tests ===" > performance_report.txt
        for i in {1..5}; do
          echo "Run $i:" >> performance_report.txt
          ./bin/benchmark_openjtalk ../test_dictionary >> performance_report.txt 2>&1
          echo "" >> performance_report.txt
        done
      shell: bash
      
    - name: Analyze Performance
      run: |
        cd Assets/uPiper/Native/OpenJTalk/build
        
        # Extract timing data
        grep -E "Average processing time|per sentence" performance_report.txt > timing_data.txt
        
        # Check if all runs meet < 10ms requirement
        if grep -q "Average processing time.*[0-9]\.[0-9]* ms" timing_data.txt; then
          avg_times=$(grep -oE "[0-9]+\.[0-9]+" timing_data.txt)
          failed=0
          
          for time in $avg_times; do
            # Extract integer part for simple comparison
            time_int=$(echo "$time" | cut -d. -f1)
            
            # If integer part is 10 or greater, it's a regression
            if [ "$time_int" -ge 10 ]; then
              echo "❌ Performance regression detected: ${time}ms >= 10ms"
              failed=1
            else
              echo "✅ Performance OK: ${time}ms < 10ms"
            fi
          done
          
          if [ $failed -eq 0 ]; then
            echo "✅ All performance tests passed (< 10ms)"
          else
            exit 1
          fi
        else
          echo "❌ Could not parse performance data"
          cat performance_report.txt
          exit 1
        fi
      shell: bash
      
    - name: Upload Performance Report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-${{ matrix.os }}
        path: |
          Assets/uPiper/Native/OpenJTalk/build/performance_report.txt
          Assets/uPiper/Native/OpenJTalk/build/timing_data.txt
        retention-days: 30

  performance-summary:
    name: Performance Summary
    needs: performance-test
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Download All Reports
      uses: actions/download-artifact@v4
      with:
        path: reports
        
    - name: Generate Summary
      run: |
        echo "# Performance Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Platform | Status | Notes |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|--------|-------|" >> $GITHUB_STEP_SUMMARY
        
        for os in ubuntu-latest macos-latest windows-latest; do
          if [ -f "reports/performance-$os/timing_data.txt" ]; then
            avg_time=$(grep -oE "[0-9]+\.[0-9]+" "reports/performance-$os/timing_data.txt" | head -1)
            if (( $(echo "$avg_time < 10" | bc -l) )); then
              echo "| $os | ✅ | ${avg_time}ms |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| $os | ❌ | ${avg_time}ms (> 10ms) |" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "| $os | ❓ | No data |" >> $GITHUB_STEP_SUMMARY
          fi
        done
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## pyopenjtalk Performance Parity" >> $GITHUB_STEP_SUMMARY
        echo "Target: < 10ms per sentence (pyopenjtalk baseline)" >> $GITHUB_STEP_SUMMARY